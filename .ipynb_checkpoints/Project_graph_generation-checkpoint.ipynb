{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Questions: Are the different coefficients uniform amongst all the nodes in the graph ? Right now, they are equal for all nodes. \n",
    "# Yes they should be equal since we are assuming that the effect is shared and we just variate over the value of the feature.\n",
    "#To easily compute and update the happiness, we keep the following properties true during the evolution of the graph:\n",
    "# - the attribute contains the happiness of the current node at each step\n",
    "# - at the begining of a step, the attribute 'happiness_updating' contains coeff_X*X + coeff_Z*Z + coeff_previous_happiness*happiness.\n",
    "#Therefore we only have to add the influence of all the other nodes at 'happiness_updating' to obtain the updated happiness.\n",
    "\n",
    "#Here, at each step, we choose a random node and add one friend to it. Then update the happiness of everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining our global variables:\n",
    "N_nodes = 10\n",
    "#This is the rate of updated nodes for each run (only 70% of the nodes will have a new friend per run here for instance)\n",
    "rate_updated_nodes_per_run = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining graph\n",
    "G=nx.DiGraph()\n",
    "G.add_nodes_from(range(N_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining attributes\n",
    "for i in range(N_nodes):\n",
    "    G.node[i]['X'] = random.random()\n",
    "    G.node[i]['Z'] = random.random()\n",
    "    G.node[i]['coeff_X'] = 1\n",
    "    G.node[i]['coeff_Z'] = 1\n",
    "    G.node[i]['coeff_previous_happiness'] = 1\n",
    "    G.node[i]['coeff_influence'] = 1\n",
    "    G.node[i]['happiness'] = G.node[i]['X'] * G.node[i]['coeff_X'] + G.node[i]['Z'] * G.node[i]['coeff_Z']\n",
    "    G.node[i]['previous_happiness'] = []\n",
    "    G.node[i]['happiness_updating'] = G.node[i]['X'] * G.node[i]['coeff_X'] + G.node[i]['Z'] * G.node[i]['coeff_Z']+ G.node[i]['coeff_previous_happiness']* G.node[i]['happiness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating similarity matrices\n",
    "similarity_matrix = [[0 for i in range(N_nodes)] for j in range(N_nodes)]\n",
    "observable_matrix = [[0 for i in range(N_nodes)] for j in range(N_nodes)]\n",
    "similarity_coefficient = 3\n",
    "for i in range(N_nodes):\n",
    "    for j in range(i):\n",
    "        similarity_matrix[i][j] = -similarity_coefficient*math.sqrt((G.node[i]['X'] - G.node[j]['X'])**2 + (G.node[i]['Z'] - G.node[j]['Z'])**2)\n",
    "        observable_matrix[i][j] = math.sqrt((G.node[i]['X'] - G.node[j]['X'])**2) \n",
    "        similarity_matrix[i][j] = 1.0 / (1.0 + math.exp(-similarity_matrix[i][j]))\n",
    "        similarity_matrix[j][i] = similarity_matrix[i][j]\n",
    "        observable_matrix[j][i] = observable_matrix[i][j]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating set of potential friends for each node\n",
    "def create_list_potential_friends(Graph):\n",
    "    list_potential_friends = [[] for i in range(N_nodes)]\n",
    "    for i in range(N_nodes):\n",
    "        for j in range(N_nodes):\n",
    "            if i != j and (i,j) not in Graph.edges():\n",
    "                if random.random() < similarity_matrix[i][j]:\n",
    "                    list_potential_friends[i].append(j)\n",
    "    return list_potential_friends\n",
    "\n",
    "global list_potential_friends\n",
    "list_potential_friends = create_list_potential_friends(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to add to the node \"node\" a random friend from his list of friends (and to remove him from the list)\n",
    "def add_one_friend(node, Graph, idx_step):\n",
    "    if len(list_potential_friends[node])>0:\n",
    "        new_friend = random.choice(list_potential_friends[node])\n",
    "        Graph.add_edge(node, new_friend, step_creation = idx_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This step is made to add a friend for each node of the Graph\n",
    "def init_step(Graph, idx_step):\n",
    "    for node in range(N_nodes):\n",
    "        add_one_friend(node, Graph, idx_step)\n",
    "    update_happiness(Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Update happiness \n",
    "def update_happiness(Graph):\n",
    "    for (a,b) in Graph.edges_iter():\n",
    "        Graph.node[a]['happiness_updating'] +=  G.node[a]['coeff_influence'] *Graph.node[b]['happiness']/len(Graph.neighbors(a))\n",
    "    for i in range(N_nodes):\n",
    "        Graph.node[i]['previous_happiness'].append(G.node[i]['happiness'])\n",
    "        Graph.node[i]['happiness'] = G.node[i]['happiness_updating']\n",
    "        Graph.node[i]['happiness_updating'] = G.node[i]['coeff_previous_happiness']*G.node[i]['happiness'] + G.node[i]['X'] * G.node[i]['coeff_X'] + G.node[i]['Z'] * G.node[i]['coeff_Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_step(Graph, idx_step):\n",
    "    global list_potential_friends\n",
    "    for node in range(N_nodes):\n",
    "        if random.random() < rate_updated_nodes_per_run:\n",
    "            add_one_friend(node, Graph, idx_step)\n",
    "    update_happiness(Graph)\n",
    "    list_potential_friends = create_list_potential_friends(Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 7, 9], [5, 6, 8], [0, 1, 6, 8], [], [6, 7, 8], [3, 6, 7, 8], [], [8], [0, 2, 9], [4]]\n"
     ]
    }
   ],
   "source": [
    "print list_potential_friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NO fixed point algorithm (convergence not guaranteed), rather use the outputs at the previous time.\n",
    "#Think about best convergence, not go through the nodes in the same way at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Main run for the graph\n",
    "nb_steps=1\n",
    "init_step(G, 0)\n",
    "for i in range(nb_steps):\n",
    "    print i\n",
    "    run_step(G, i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': 0.7043841502709461,\n",
       " 'Z': 0.39395264928483986,\n",
       " 'coeff_X': 1,\n",
       " 'coeff_Z': 1,\n",
       " 'coeff_influence': 1,\n",
       " 'coeff_previous_happiness': 1,\n",
       " 'happiness': 6.537317874677609,\n",
       " 'happiness_updating': 7.635654674233395,\n",
       " 'previous_happiness': [1.098336799555786, 3.1087579289222043]}"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for e in G.edges():\n",
    "#     print e['step_creation']\n",
    "# nx.get_edge_attributes(G, \"step_creation\")\n",
    "G.node[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFBCAYAAACvlHzeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiICVTAIiIiJlABi4iImOD/AfmDuAMdcbYiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10627e650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the last graph obtained\n",
    "nx.draw(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_propensity_table(Graph):\n",
    "    edges = []\n",
    "    treatment = []\n",
    "    response = []\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    for i in range(N_nodes):\n",
    "        for j in range(i+1,N_nodes):\n",
    "            if Graph.node[j]['previous_happiness'][0]> Graph.node[i]['previous_happiness'][0]:\n",
    "                u,v = i,j\n",
    "            else:\n",
    "                u,v = j,i\n",
    "                \n",
    "            if (u,v) in Graph.edges():\n",
    "                treatment.append(1)\n",
    "                step_creation = Graph[u][v]['step_creation']\n",
    "                edges.append((u,v))\n",
    "                response.append(Graph.node[u]['happiness'] - Graph.node[u]['previous_happiness'][0])\n",
    "                X1.append(Graph.node[u]['X'])\n",
    "                X2.append(Graph.node[v]['X'])\n",
    "            elif len(Graph.neighbors(u))==1 :\n",
    "                edges.append((u,v))\n",
    "                treatment.append(0)\n",
    "                response.append(Graph.node[u]['happiness'] - Graph.node[u]['previous_happiness'][0])\n",
    "                X1.append(Graph.node[u]['X'])\n",
    "                X2.append(Graph.node[v]['X'])\n",
    "                \n",
    "            \n",
    "                \n",
    "    return pd.DataFrame({'Edges':edges,\n",
    "                    'Treatment': treatment,\n",
    "                    'Response': response,\n",
    "                    'X1': X1,\n",
    "                    'X2': X2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Edges</th>\n",
       "      <th>Response</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0)</td>\n",
       "      <td>5.174188</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719740</td>\n",
       "      <td>0.963168</td>\n",
       "      <td>0.243428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(7, 0)</td>\n",
       "      <td>6.075953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647808</td>\n",
       "      <td>0.963168</td>\n",
       "      <td>0.315360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(8, 0)</td>\n",
       "      <td>7.984245</td>\n",
       "      <td>1</td>\n",
       "      <td>0.595565</td>\n",
       "      <td>0.963168</td>\n",
       "      <td>0.367603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(9, 0)</td>\n",
       "      <td>8.005471</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438465</td>\n",
       "      <td>0.963168</td>\n",
       "      <td>0.524703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(7, 1)</td>\n",
       "      <td>6.075953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647808</td>\n",
       "      <td>0.704384</td>\n",
       "      <td>0.056576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(4, 2)</td>\n",
       "      <td>5.174188</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719740</td>\n",
       "      <td>0.946739</td>\n",
       "      <td>0.226999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(7, 2)</td>\n",
       "      <td>6.075953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647808</td>\n",
       "      <td>0.946739</td>\n",
       "      <td>0.298931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(9, 2)</td>\n",
       "      <td>8.005471</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438465</td>\n",
       "      <td>0.946739</td>\n",
       "      <td>0.508274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>3.878941</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608738</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>0.381011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(7, 3)</td>\n",
       "      <td>6.075953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647808</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>0.341941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(7, 4)</td>\n",
       "      <td>6.075953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647808</td>\n",
       "      <td>0.719740</td>\n",
       "      <td>0.071932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(9, 4)</td>\n",
       "      <td>8.005471</td>\n",
       "      <td>1</td>\n",
       "      <td>0.438465</td>\n",
       "      <td>0.719740</td>\n",
       "      <td>0.281275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>3.878941</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608738</td>\n",
       "      <td>0.319825</td>\n",
       "      <td>0.288913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(7, 9)</td>\n",
       "      <td>6.075953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647808</td>\n",
       "      <td>0.438465</td>\n",
       "      <td>0.209343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>7.984245</td>\n",
       "      <td>1</td>\n",
       "      <td>0.595565</td>\n",
       "      <td>0.438465</td>\n",
       "      <td>0.157101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Edges  Response  Treatment        X1        X2        X3\n",
       "0   (4, 0)  5.174188          0  0.719740  0.963168  0.243428\n",
       "1   (7, 0)  6.075953          0  0.647808  0.963168  0.315360\n",
       "2   (8, 0)  7.984245          1  0.595565  0.963168  0.367603\n",
       "3   (9, 0)  8.005471          0  0.438465  0.963168  0.524703\n",
       "4   (7, 1)  6.075953          0  0.647808  0.704384  0.056576\n",
       "5   (4, 2)  5.174188          0  0.719740  0.946739  0.226999\n",
       "6   (7, 2)  6.075953          0  0.647808  0.946739  0.298931\n",
       "7   (9, 2)  8.005471          0  0.438465  0.946739  0.508274\n",
       "8   (5, 3)  3.878941          1  0.608738  0.989749  0.381011\n",
       "9   (7, 3)  6.075953          0  0.647808  0.989749  0.341941\n",
       "10  (7, 4)  6.075953          0  0.647808  0.719740  0.071932\n",
       "11  (9, 4)  8.005471          1  0.438465  0.719740  0.281275\n",
       "12  (5, 6)  3.878941          1  0.608738  0.319825  0.288913\n",
       "13  (7, 9)  6.075953          0  0.647808  0.438465  0.209343\n",
       "14  (8, 9)  7.984245          1  0.595565  0.438465  0.157101"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = create_propensity_table(G)\n",
    "data['X3'] = abs(data['X2']-data['X1'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "propensity = LogisticRegression()\n",
    "propensity = propensity.fit(data[['X1','X2','X3']], data.Treatment)\n",
    "pscore = propensity.predict_proba(data[['X1','X2','X3']])[:,1] # The predicted propensities by the model\n",
    "data['Propensity'] = pscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Edges</th>\n",
       "      <th>Response</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>Propensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0)</td>\n",
       "      <td>5.174188</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719740</td>\n",
       "      <td>0.963168</td>\n",
       "      <td>0.243428</td>\n",
       "      <td>0.313349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(7, 0)</td>\n",
       "      <td>6.075953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647808</td>\n",
       "      <td>0.963168</td>\n",
       "      <td>0.315360</td>\n",
       "      <td>0.317472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(8, 0)</td>\n",
       "      <td>7.984245</td>\n",
       "      <td>1</td>\n",
       "      <td>0.595565</td>\n",
       "      <td>0.963168</td>\n",
       "      <td>0.367603</td>\n",
       "      <td>0.320485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(9, 0)</td>\n",
       "      <td>8.005471</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438465</td>\n",
       "      <td>0.963168</td>\n",
       "      <td>0.524703</td>\n",
       "      <td>0.329635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(7, 1)</td>\n",
       "      <td>6.075953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647808</td>\n",
       "      <td>0.704384</td>\n",
       "      <td>0.056576</td>\n",
       "      <td>0.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(4, 2)</td>\n",
       "      <td>5.174188</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719740</td>\n",
       "      <td>0.946739</td>\n",
       "      <td>0.226999</td>\n",
       "      <td>0.315170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(7, 2)</td>\n",
       "      <td>6.075953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647808</td>\n",
       "      <td>0.946739</td>\n",
       "      <td>0.298931</td>\n",
       "      <td>0.319307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(9, 2)</td>\n",
       "      <td>8.005471</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438465</td>\n",
       "      <td>0.946739</td>\n",
       "      <td>0.508274</td>\n",
       "      <td>0.331506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>3.878941</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608738</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>0.381011</td>\n",
       "      <td>0.316757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(7, 3)</td>\n",
       "      <td>6.075953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647808</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>0.341941</td>\n",
       "      <td>0.314516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(7, 4)</td>\n",
       "      <td>6.075953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647808</td>\n",
       "      <td>0.719740</td>\n",
       "      <td>0.071932</td>\n",
       "      <td>0.345212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(9, 4)</td>\n",
       "      <td>8.005471</td>\n",
       "      <td>1</td>\n",
       "      <td>0.438465</td>\n",
       "      <td>0.719740</td>\n",
       "      <td>0.281275</td>\n",
       "      <td>0.357881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>3.878941</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608738</td>\n",
       "      <td>0.319825</td>\n",
       "      <td>0.288913</td>\n",
       "      <td>0.401545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(7, 9)</td>\n",
       "      <td>6.075953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647808</td>\n",
       "      <td>0.438465</td>\n",
       "      <td>0.209343</td>\n",
       "      <td>0.382883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>7.984245</td>\n",
       "      <td>1</td>\n",
       "      <td>0.595565</td>\n",
       "      <td>0.438465</td>\n",
       "      <td>0.157101</td>\n",
       "      <td>0.385096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Edges  Response  Treatment        X1        X2        X3  Propensity\n",
       "0   (4, 0)  5.174188          0  0.719740  0.963168  0.243428    0.313349\n",
       "1   (7, 0)  6.075953          0  0.647808  0.963168  0.315360    0.317472\n",
       "2   (8, 0)  7.984245          1  0.595565  0.963168  0.367603    0.320485\n",
       "3   (9, 0)  8.005471          0  0.438465  0.963168  0.524703    0.329635\n",
       "4   (7, 1)  6.075953          0  0.647808  0.704384  0.056576    0.347000\n",
       "5   (4, 2)  5.174188          0  0.719740  0.946739  0.226999    0.315170\n",
       "6   (7, 2)  6.075953          0  0.647808  0.946739  0.298931    0.319307\n",
       "7   (9, 2)  8.005471          0  0.438465  0.946739  0.508274    0.331506\n",
       "8   (5, 3)  3.878941          1  0.608738  0.989749  0.381011    0.316757\n",
       "9   (7, 3)  6.075953          0  0.647808  0.989749  0.341941    0.314516\n",
       "10  (7, 4)  6.075953          0  0.647808  0.719740  0.071932    0.345212\n",
       "11  (9, 4)  8.005471          1  0.438465  0.719740  0.281275    0.357881\n",
       "12  (5, 6)  3.878941          1  0.608738  0.319825  0.288913    0.401545\n",
       "13  (7, 9)  6.075953          0  0.647808  0.438465  0.209343    0.382883\n",
       "14  (8, 9)  7.984245          1  0.595565  0.438465  0.157101    0.385096"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Match(groups, propensity, caliper = 0.05):\n",
    "    ''' \n",
    "    Inputs:\n",
    "    groups = Treatment assignments.  Must be 2 groups\n",
    "    propensity = Propensity scores for each observation. Propensity and groups should be in the same order (matching indices)\n",
    "    caliper = Maximum difference in matched propensity scores. For now, this is a caliper on the raw\n",
    "            propensity; Austin reccommends using a caliper on the logit propensity.\n",
    "    \n",
    "    Output:\n",
    "    A series containing the individuals in the control group matched to the treatment group.\n",
    "    Note that with caliper matching, not every treated individual may have a match.\n",
    "    '''\n",
    "\n",
    "    # Check inputs\n",
    "    if any(propensity <=0) or any(propensity >=1):\n",
    "        raise ValueError('Propensity scores must be between 0 and 1')\n",
    "    elif not(0<caliper<1):\n",
    "        raise ValueError('Caliper must be between 0 and 1')\n",
    "    elif len(groups)!= len(propensity):\n",
    "        raise ValueError('groups and propensity scores must be same dimension')\n",
    "    elif len(groups.unique()) != 2:\n",
    "        raise ValueError('wrong number of groups')\n",
    "        \n",
    "        \n",
    "    # Code groups as 0 and 1\n",
    "    groups = groups == groups.unique()[0]\n",
    "    N = len(groups)\n",
    "    N1 = groups.sum(); N2 = N-N1\n",
    "    g1, g2 = propensity[groups == 1].reset_index(drop=True), (propensity[groups == 0]).reset_index(drop=True)\n",
    "    # Check if treatment groups got flipped - treatment (coded 1) should be the smaller\n",
    "    if N1 > N2:\n",
    "        N1, N2, g1, g2 = N2, N1, g2, g1 \n",
    "        \n",
    "        \n",
    "    # Randomly permute the smaller group to get order for matching\n",
    "    morder = np.random.permutation(N1)\n",
    "    matches = pd.Series(np.empty(N1))\n",
    "    matches[:] = np.NAN\n",
    "    \n",
    "    for m in morder:\n",
    "        dist = abs(g1[m] - g2)\n",
    "        if dist.min() <= caliper:\n",
    "            matches[m] = int(dist.argmin())\n",
    "            g2 = g2.drop(matches[m])\n",
    "    return (matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5\n",
       "1    1\n",
       "2    8\n",
       "3    9\n",
       "4    3\n",
       "dtype: float64"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff = Match(data.Treatment, data.Propensity)\n",
    "stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3204851431423123, 0.31517040681148017),\n",
       " (0.31675651665058341, 0.31747216074458123),\n",
       " (0.35788065551775122, nan),\n",
       " (0.40154537724418427, 0.31451590137432667),\n",
       " (0.38509630112870957, 0.32963545684700929)]"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1, g2 = data.Propensity[data.Treatment==1], data.Propensity[data.Treatment==0]\n",
    "zip(g1, g2[stuff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Edges</th>\n",
       "      <th>Response</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>Propensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(8, 0)</td>\n",
       "      <td>7.984245</td>\n",
       "      <td>1</td>\n",
       "      <td>0.595565</td>\n",
       "      <td>0.963168</td>\n",
       "      <td>0.367603</td>\n",
       "      <td>0.320485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>3.878941</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608738</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>0.381011</td>\n",
       "      <td>0.316757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(9, 4)</td>\n",
       "      <td>8.005471</td>\n",
       "      <td>1</td>\n",
       "      <td>0.438465</td>\n",
       "      <td>0.719740</td>\n",
       "      <td>0.281275</td>\n",
       "      <td>0.357881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>3.878941</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608738</td>\n",
       "      <td>0.319825</td>\n",
       "      <td>0.288913</td>\n",
       "      <td>0.401545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>7.984245</td>\n",
       "      <td>1</td>\n",
       "      <td>0.595565</td>\n",
       "      <td>0.438465</td>\n",
       "      <td>0.157101</td>\n",
       "      <td>0.385096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Edges  Response  Treatment        X1        X2        X3  Propensity\n",
       "2   (8, 0)  7.984245          1  0.595565  0.963168  0.367603    0.320485\n",
       "8   (5, 3)  3.878941          1  0.608738  0.989749  0.381011    0.316757\n",
       "11  (9, 4)  8.005471          1  0.438465  0.719740  0.281275    0.357881\n",
       "12  (5, 6)  3.878941          1  0.608738  0.319825  0.288913    0.401545\n",
       "14  (8, 9)  7.984245          1  0.595565  0.438465  0.157101    0.385096"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = data[data.Treatment==1]\n",
    "st=[int(x) for x in stuff]\n",
    "data2 = pd.concat([data.iloc[st],r1])\n",
    "data2\n",
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = sm.ols(formula=\"Response ~ Treatment + X1\", data=data2).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    14.003906\n",
       "Treatment    -0.866520\n",
       "X1          -12.504610\n",
       "dtype: float64"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
